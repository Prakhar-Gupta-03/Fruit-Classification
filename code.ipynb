{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data from Train Dataset\n",
    "Features, Labels = [], []\n",
    "with open ('train.csv', mode='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    data = list(csvFile)\n",
    "    data = data[1:]\n",
    "    for data_point in data:\n",
    "        Features.append(np.array([float(x) for x in data_point[1:-1]]))\n",
    "        Labels.append(data_point[-1])\n",
    "Features = np.array(Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data from Test Dataset\n",
    "Features_Test = []\n",
    "with open ('test.csv', mode='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    data = list(csvFile)\n",
    "    data = data[1:]\n",
    "    for data_point in data:\n",
    "        Features_Test.append(np.array([float(x) for x in data_point[1:]]))\n",
    "Features_Test = np.array(Features_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_Train, Features_Validation, Labels_Train, Labels_Validation = train_test_split(Features, Labels, test_size=0.1, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clf = LogisticRegression()\n",
    "# clf.fit(Features_Train_PCA, Labels_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.11475409836065574\n",
      "2 0.16393442622950818\n",
      "3 0.20491803278688525\n",
      "4 0.26229508196721313\n",
      "5 0.3442622950819672\n",
      "6 0.39344262295081966\n",
      "7 0.4344262295081967\n",
      "8 0.4262295081967213\n",
      "9 0.5245901639344263\n",
      "10 0.5409836065573771\n",
      "11 0.5655737704918032\n",
      "12 0.5491803278688525\n",
      "13 0.5491803278688525\n",
      "14 0.5901639344262295\n",
      "15 0.5901639344262295\n",
      "16 0.5901639344262295\n",
      "17 0.6147540983606558\n",
      "18 0.6065573770491803\n",
      "19 0.6229508196721312\n",
      "20 0.6229508196721312\n",
      "21 0.6147540983606558\n",
      "22 0.6065573770491803\n",
      "23 0.639344262295082\n",
      "24 0.6311475409836066\n",
      "25 0.639344262295082\n",
      "26 0.6475409836065574\n",
      "27 0.6557377049180327\n",
      "28 0.639344262295082\n",
      "29 0.6475409836065574\n",
      "30 0.6475409836065574\n",
      "31 0.6475409836065574\n",
      "32 0.6721311475409836\n",
      "33 0.6721311475409836\n",
      "34 0.680327868852459\n",
      "35 0.680327868852459\n",
      "36 0.6967213114754098\n",
      "37 0.6885245901639344\n",
      "38 0.6967213114754098\n",
      "39 0.7131147540983607\n",
      "40 0.7213114754098361\n",
      "41 0.7213114754098361\n",
      "42 0.7213114754098361\n",
      "43 0.7213114754098361\n",
      "44 0.7213114754098361\n",
      "45 0.7213114754098361\n",
      "46 0.7459016393442623\n",
      "47 0.7459016393442623\n",
      "48 0.7377049180327869\n",
      "49 0.7540983606557377\n",
      "50 0.7295081967213115\n",
      "51 0.7295081967213115\n",
      "52 0.7295081967213115\n",
      "53 0.7377049180327869\n",
      "54 0.7295081967213115\n",
      "55 0.7295081967213115\n",
      "56 0.7295081967213115\n",
      "57 0.7377049180327869\n",
      "58 0.7295081967213115\n",
      "59 0.7213114754098361\n",
      "60 0.7213114754098361\n",
      "61 0.7213114754098361\n",
      "62 0.7213114754098361\n",
      "63 0.7295081967213115\n",
      "64 0.7213114754098361\n",
      "65 0.7213114754098361\n",
      "66 0.7295081967213115\n",
      "67 0.7377049180327869\n",
      "68 0.7295081967213115\n",
      "69 0.7377049180327869\n",
      "70 0.7377049180327869\n",
      "71 0.7295081967213115\n",
      "72 0.7295081967213115\n",
      "73 0.7377049180327869\n",
      "74 0.7540983606557377\n",
      "75 0.7540983606557377\n",
      "76 0.7377049180327869\n",
      "77 0.7459016393442623\n",
      "78 0.7377049180327869\n",
      "79 0.7540983606557377\n",
      "80 0.7540983606557377\n",
      "81 0.7622950819672131\n",
      "82 0.7622950819672131\n",
      "83 0.7459016393442623\n",
      "84 0.7540983606557377\n",
      "85 0.7622950819672131\n",
      "86 0.7704918032786885\n",
      "87 0.7622950819672131\n",
      "88 0.7540983606557377\n",
      "89 0.7540983606557377\n",
      "90 0.7377049180327869\n",
      "91 0.7786885245901639\n",
      "92 0.7704918032786885\n",
      "93 0.7622950819672131\n",
      "94 0.7704918032786885\n",
      "95 0.7786885245901639\n",
      "96 0.7704918032786885\n",
      "97 0.7786885245901639\n",
      "98 0.7786885245901639\n",
      "99 0.7786885245901639\n",
      "100 0.7704918032786885\n",
      "101 0.7786885245901639\n",
      "102 0.7868852459016393\n",
      "103 0.7786885245901639\n",
      "104 0.7868852459016393\n",
      "105 0.7950819672131147\n",
      "106 0.8032786885245902\n",
      "107 0.7950819672131147\n",
      "108 0.7868852459016393\n",
      "109 0.8032786885245902\n",
      "110 0.7786885245901639\n",
      "111 0.8032786885245902\n",
      "112 0.7950819672131147\n",
      "113 0.7950819672131147\n",
      "114 0.7786885245901639\n",
      "115 0.7950819672131147\n",
      "116 0.7868852459016393\n",
      "117 0.7950819672131147\n",
      "118 0.7868852459016393\n",
      "119 0.7704918032786885\n",
      "120 0.7786885245901639\n",
      "121 0.7868852459016393\n",
      "122 0.7868852459016393\n",
      "123 0.7786885245901639\n",
      "124 0.7786885245901639\n",
      "125 0.7786885245901639\n",
      "126 0.8032786885245902\n",
      "127 0.7786885245901639\n",
      "128 0.7786885245901639\n",
      "129 0.7786885245901639\n",
      "130 0.7868852459016393\n",
      "131 0.7704918032786885\n",
      "132 0.7622950819672131\n",
      "133 0.7868852459016393\n",
      "134 0.7868852459016393\n",
      "135 0.7786885245901639\n",
      "136 0.7868852459016393\n",
      "137 0.7704918032786885\n",
      "138 0.7868852459016393\n",
      "139 0.7786885245901639\n",
      "140 0.7786885245901639\n",
      "141 0.7786885245901639\n",
      "142 0.7786885245901639\n",
      "143 0.7704918032786885\n",
      "144 0.7786885245901639\n",
      "145 0.7950819672131147\n",
      "146 0.7950819672131147\n",
      "147 0.7786885245901639\n",
      "148 0.7704918032786885\n",
      "149 0.7704918032786885\n",
      "150 0.7868852459016393\n",
      "151 0.7704918032786885\n",
      "152 0.7868852459016393\n",
      "153 0.7950819672131147\n",
      "154 0.7950819672131147\n",
      "155 0.7786885245901639\n",
      "156 0.7868852459016393\n",
      "157 0.7786885245901639\n",
      "158 0.7786885245901639\n",
      "159 0.7868852459016393\n",
      "160 0.7622950819672131\n",
      "161 0.7868852459016393\n",
      "162 0.7950819672131147\n",
      "163 0.7786885245901639\n",
      "164 0.7950819672131147\n",
      "165 0.7950819672131147\n",
      "166 0.7704918032786885\n",
      "167 0.7786885245901639\n",
      "168 0.7950819672131147\n",
      "169 0.7786885245901639\n",
      "170 0.7868852459016393\n",
      "171 0.7950819672131147\n",
      "172 0.8032786885245902\n",
      "173 0.7950819672131147\n",
      "174 0.7950819672131147\n",
      "175 0.7950819672131147\n",
      "176 0.8032786885245902\n",
      "177 0.7868852459016393\n",
      "178 0.7786885245901639\n",
      "179 0.7950819672131147\n",
      "180 0.7950819672131147\n",
      "181 0.8032786885245902\n",
      "182 0.7786885245901639\n",
      "183 0.7868852459016393\n",
      "184 0.8114754098360656\n",
      "185 0.8032786885245902\n",
      "186 0.7950819672131147\n",
      "187 0.8032786885245902\n",
      "188 0.7950819672131147\n",
      "189 0.8114754098360656\n",
      "190 0.8114754098360656\n",
      "191 0.7950819672131147\n",
      "192 0.8114754098360656\n",
      "193 0.8032786885245902\n",
      "194 0.8114754098360656\n",
      "195 0.8032786885245902\n",
      "196 0.8032786885245902\n",
      "197 0.8114754098360656\n",
      "198 0.8032786885245902\n",
      "199 0.8114754098360656\n",
      "200 0.8114754098360656\n",
      "201 0.819672131147541\n",
      "202 0.8114754098360656\n",
      "203 0.8278688524590164\n",
      "204 0.8114754098360656\n",
      "205 0.8114754098360656\n",
      "206 0.8032786885245902\n",
      "207 0.8114754098360656\n",
      "208 0.8360655737704918\n",
      "209 0.819672131147541\n",
      "210 0.8360655737704918\n",
      "211 0.8114754098360656\n",
      "212 0.819672131147541\n",
      "213 0.819672131147541\n",
      "214 0.7950819672131147\n",
      "215 0.8032786885245902\n",
      "216 0.8032786885245902\n",
      "217 0.819672131147541\n",
      "218 0.8114754098360656\n",
      "219 0.8114754098360656\n",
      "220 0.819672131147541\n",
      "221 0.8360655737704918\n",
      "222 0.8360655737704918\n",
      "223 0.8278688524590164\n",
      "224 0.819672131147541\n",
      "225 0.8278688524590164\n",
      "226 0.819672131147541\n",
      "227 0.8360655737704918\n",
      "228 0.8114754098360656\n",
      "229 0.8360655737704918\n",
      "230 0.8114754098360656\n",
      "231 0.819672131147541\n",
      "232 0.819672131147541\n",
      "233 0.819672131147541\n",
      "234 0.8278688524590164\n",
      "235 0.8360655737704918\n",
      "236 0.8360655737704918\n",
      "237 0.819672131147541\n",
      "238 0.8278688524590164\n",
      "239 0.819672131147541\n",
      "240 0.8360655737704918\n",
      "241 0.8360655737704918\n",
      "242 0.8278688524590164\n",
      "243 0.8360655737704918\n",
      "244 0.8114754098360656\n",
      "245 0.8278688524590164\n",
      "246 0.8278688524590164\n",
      "247 0.8032786885245902\n",
      "248 0.8032786885245902\n",
      "249 0.8360655737704918\n",
      "250 0.8278688524590164\n",
      "251 0.8442622950819673\n",
      "252 0.8278688524590164\n",
      "253 0.8360655737704918\n",
      "254 0.8032786885245902\n",
      "255 0.8360655737704918\n",
      "256 0.8360655737704918\n",
      "257 0.8360655737704918\n",
      "258 0.8278688524590164\n",
      "259 0.8278688524590164\n",
      "260 0.8114754098360656\n",
      "261 0.8360655737704918\n",
      "262 0.8360655737704918\n",
      "263 0.8032786885245902\n",
      "264 0.8442622950819673\n",
      "265 0.8114754098360656\n",
      "266 0.8278688524590164\n",
      "267 0.8114754098360656\n",
      "268 0.8278688524590164\n",
      "269 0.860655737704918\n",
      "270 0.819672131147541\n",
      "271 0.8114754098360656\n",
      "272 0.8360655737704918\n",
      "273 0.8114754098360656\n",
      "274 0.8360655737704918\n",
      "275 0.819672131147541\n",
      "276 0.8442622950819673\n",
      "277 0.819672131147541\n",
      "278 0.819672131147541\n",
      "279 0.8360655737704918\n",
      "280 0.8278688524590164\n",
      "281 0.819672131147541\n",
      "282 0.819672131147541\n",
      "283 0.8278688524590164\n",
      "284 0.8442622950819673\n",
      "285 0.819672131147541\n",
      "286 0.819672131147541\n",
      "287 0.8360655737704918\n",
      "288 0.8442622950819673\n",
      "289 0.8278688524590164\n",
      "290 0.819672131147541\n",
      "291 0.8278688524590164\n",
      "292 0.8032786885245902\n",
      "293 0.8360655737704918\n",
      "294 0.8360655737704918\n",
      "295 0.8442622950819673\n",
      "296 0.8278688524590164\n",
      "297 0.819672131147541\n",
      "298 0.8114754098360656\n",
      "299 0.8442622950819673\n",
      "300 0.8442622950819673\n",
      "301 0.8524590163934426\n",
      "302 0.8360655737704918\n",
      "303 0.8442622950819673\n",
      "304 0.8442622950819673\n",
      "305 0.8360655737704918\n",
      "306 0.8360655737704918\n",
      "307 0.819672131147541\n",
      "308 0.8442622950819673\n",
      "309 0.8442622950819673\n",
      "310 0.8442622950819673\n",
      "311 0.8278688524590164\n",
      "312 0.8278688524590164\n",
      "313 0.8278688524590164\n",
      "314 0.8360655737704918\n",
      "315 0.8360655737704918\n",
      "316 0.8278688524590164\n",
      "317 0.8360655737704918\n",
      "318 0.819672131147541\n",
      "319 0.8360655737704918\n",
      "320 0.8442622950819673\n",
      "321 0.8360655737704918\n",
      "322 0.8360655737704918\n",
      "323 0.8442622950819673\n",
      "324 0.8442622950819673\n",
      "325 0.8278688524590164\n",
      "326 0.8442622950819673\n",
      "327 0.8360655737704918\n",
      "328 0.8360655737704918\n",
      "329 0.8278688524590164\n",
      "330 0.8360655737704918\n",
      "331 0.8442622950819673\n",
      "332 0.8278688524590164\n",
      "333 0.8442622950819673\n",
      "334 0.8442622950819673\n",
      "335 0.8442622950819673\n",
      "336 0.8360655737704918\n",
      "337 0.8442622950819673\n",
      "338 0.8442622950819673\n",
      "339 0.8524590163934426\n",
      "340 0.8278688524590164\n",
      "341 0.8360655737704918\n",
      "342 0.8442622950819673\n",
      "343 0.8360655737704918\n",
      "344 0.8360655737704918\n",
      "345 0.8442622950819673\n",
      "346 0.8524590163934426\n",
      "347 0.8524590163934426\n",
      "348 0.8524590163934426\n",
      "349 0.8524590163934426\n",
      "350 0.819672131147541\n",
      "351 0.8442622950819673\n",
      "352 0.8442622950819673\n",
      "353 0.8360655737704918\n",
      "354 0.8360655737704918\n",
      "355 0.8442622950819673\n",
      "356 0.8442622950819673\n",
      "357 0.8442622950819673\n",
      "358 0.8360655737704918\n",
      "359 0.8442622950819673\n",
      "360 0.8524590163934426\n",
      "361 0.8524590163934426\n",
      "362 0.8360655737704918\n",
      "363 0.8524590163934426\n",
      "364 0.8442622950819673\n",
      "365 0.8360655737704918\n",
      "366 0.8360655737704918\n",
      "367 0.8442622950819673\n",
      "368 0.8278688524590164\n",
      "369 0.8360655737704918\n",
      "370 0.8524590163934426\n",
      "371 0.8442622950819673\n",
      "372 0.8442622950819673\n",
      "373 0.8442622950819673\n",
      "374 0.8442622950819673\n",
      "375 0.8524590163934426\n",
      "376 0.8442622950819673\n",
      "377 0.8442622950819673\n",
      "378 0.8442622950819673\n",
      "379 0.8360655737704918\n",
      "380 0.8278688524590164\n",
      "381 0.8442622950819673\n",
      "382 0.8360655737704918\n",
      "383 0.8442622950819673\n",
      "384 0.8360655737704918\n",
      "385 0.8442622950819673\n",
      "386 0.8360655737704918\n",
      "387 0.8442622950819673\n",
      "388 0.8442622950819673\n",
      "389 0.8360655737704918\n",
      "390 0.8442622950819673\n",
      "391 0.8360655737704918\n",
      "392 0.8360655737704918\n",
      "393 0.8360655737704918\n",
      "394 0.8278688524590164\n",
      "395 0.8442622950819673\n",
      "396 0.8442622950819673\n",
      "397 0.8442622950819673\n",
      "398 0.8360655737704918\n",
      "399 0.8360655737704918\n",
      "400 0.8360655737704918\n",
      "401 0.8442622950819673\n",
      "402 0.8442622950819673\n",
      "403 0.8524590163934426\n",
      "404 0.8442622950819673\n",
      "405 0.8278688524590164\n",
      "406 0.8278688524590164\n",
      "407 0.8360655737704918\n",
      "408 0.8360655737704918\n",
      "409 0.8278688524590164\n",
      "410 0.8360655737704918\n",
      "411 0.8278688524590164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m Features_Train_PCA \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(Features_Train)\n\u001b[0;32m      5\u001b[0m lda \u001b[39m=\u001b[39m LinearDiscriminantAnalysis(n_components\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m X_train \u001b[39m=\u001b[39m lda\u001b[39m.\u001b[39;49mfit_transform(Features_Train_PCA, Labels_Train)\n\u001b[0;32m      7\u001b[0m \u001b[39m# print(X_train)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# X_test = lda.transform(Features_Validation)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m predict \u001b[39m=\u001b[39m lda\u001b[39m.\u001b[39mpredict(pca\u001b[39m.\u001b[39mtransform(Features_Validation))\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:881\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\discriminant_analysis.py:622\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovariance_estimator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    617\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    618\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcovariance estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis not supported \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith svd solver. Try another solver\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m         )\n\u001b[1;32m--> 622\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_solve_svd(X, y)\n\u001b[0;32m    623\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlsqr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    624\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_solve_lstsq(\n\u001b[0;32m    625\u001b[0m         X,\n\u001b[0;32m    626\u001b[0m         y,\n\u001b[0;32m    627\u001b[0m         shrinkage\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshrinkage,\n\u001b[0;32m    628\u001b[0m         covariance_estimator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovariance_estimator,\n\u001b[0;32m    629\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\discriminant_analysis.py:518\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis._solve_svd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    516\u001b[0m X \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39msqrt(fac) \u001b[39m*\u001b[39m (Xc \u001b[39m/\u001b[39m std)\n\u001b[0;32m    517\u001b[0m \u001b[39m# SVD of centered (within)scaled data\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m U, S, Vt \u001b[39m=\u001b[39m svd(X, full_matrices\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    520\u001b[0m rank \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39msum(xp\u001b[39m.\u001b[39mastype(S \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol, xp\u001b[39m.\u001b[39mint32))\n\u001b[0;32m    521\u001b[0m \u001b[39m# Scaling of within covariance is: V' 1/S\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\linalg\\_decomp_svd.py:130\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m# perform decomposition\u001b[39;00m\n\u001b[0;32m    127\u001b[0m u, s, v, info \u001b[39m=\u001b[39m gesXd(a1, compute_uv\u001b[39m=\u001b[39mcompute_uv, lwork\u001b[39m=\u001b[39mlwork,\n\u001b[0;32m    128\u001b[0m                       full_matrices\u001b[39m=\u001b[39mfull_matrices, overwrite_a\u001b[39m=\u001b[39moverwrite_a)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[0;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSVD did not converge\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "acc = []\n",
    "for i in range(1, 1000):\n",
    "    pca = PCA(n_components=i)\n",
    "    Features_Train_PCA = pca.fit_transform(Features_Train)\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    X_train = lda.fit_transform(Features_Train_PCA, Labels_Train)\n",
    "    # print(X_train)\n",
    "    # X_test = lda.transform(Features_Validation)\n",
    "    predict = lda.predict(pca.transform(Features_Validation))\n",
    "    print(i, accuracy_score(Labels_Validation, predict))\n",
    "    acc.append(accuracy_score(Labels_Validation, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860655737704918\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "print(acc[np.argmax(acc)])\n",
    "print(np.argmax(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410 0.8360655737704918\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=289)\n",
    "Features_Train_PCA = pca.fit_transform(Features_Train)\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_train = lda.fit_transform(Features_Train_PCA, Labels_Train)\n",
    "    # print(X_train)\n",
    "    # X_test = lda.transform(Features_Validation)\n",
    "predict = lda.predict(pca.transform(Features_Validation))\n",
    "print(i, accuracy_score(Labels_Validation, predict))\n",
    "# acc.append(accuracy_score(Labels_Validation, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression()\n",
    "# clf.fit(X_train, Labels_Train)\n",
    "# predict = clf.predict(X_test)\n",
    "# print(accuracy_score(Labels_Validation, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lda.predict(pca.transform(Features_Test))\n",
    "L = []\n",
    "for i in range(len(predict)):\n",
    "    L.append([i, predict[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['ID', 'Category']\n",
    "with open('submission.csv', 'w', newline='') as csvFile:\n",
    "    csvwriter = csv.writer(csvFile)\n",
    "    csvwriter.writerow(fields)\n",
    "    csvwriter.writerows(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(accuracy[np.argmax(accuracy)], components[np.argmax(accuracy)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
